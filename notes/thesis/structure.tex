\documentclass[12pt]{report}

\bibliographystyle{alpha}
\usepackage{dirtytalk}
\usepackage[none]{hyphenat}  % disables all hyphenation
\sloppy                      % allows LaTeX to adjust spacing to avoid overfull lines

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[style=verbose]{biblatex}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{array}
\usepackage[a-1b]{pdfx}
\addbibresource{references.bib}

\geometry{a4paper, margin=1in}

\newcommand{\utterance}[3]{%
    \textbf{#1} #2%
    \ifx\relax#3\relax%
    \else \\ \textit{#3}%
    \fi%
}

\begin{document}



\thispagestyle{empty}

\begin{center}
    Charles University\\
    Faculty of Arts\\
    Institute of Czech Language and Theory of Information
\end{center}

\vfill

\begin{center}
    \textbf{\Large Coherence disruptions in human–chatbot interaction}\\
    Narušení koherence v interakci člověka s chatbotem\\[1cm]
    Master thesis
\end{center}

\vfill
\begin{flushleft}
    Thesis supervisor: Mgr. Anna Nedoluzhko, Ph.D.
\end{flushleft}


\noindent
\begin{minipage}{0.5\textwidth}
    Prague 2025
\end{minipage}%
\hfill
\begin{minipage}{0.4\textwidth}
    \raggedleft Bc. Albert Maršík
\end{minipage}

\newpage
\thispagestyle{empty}
\mbox{}
\newpage
\thispagestyle{empty}

   % TODO
   % touch up
   % go through references

   \vbox{}
   \vfill
   \section*{Acknowledgments}
   I would like to thank my supervisor, Anna Nedoluzhko, for her guidance and support throughout my research.
   I also appreciate Ondřej Dušek for his valuable advice,
   which helped shape the direction of this thesis.
   Lastly, I am grateful to the guarantor of my study program, Jan Chromý,
   for fostering an environment that allowed me the freedom to follow my intuition and explore my research interest.
   \newpage
   \thispagestyle{empty}

   \vbox{}
   \vfill
   \section*{Declaration of Originality}
   I hereby declare that I have completed this work independently,
   that all sources and references used have been properly cited,
   and that this work has not been submitted as part of fulfilling any other academic requirements,
   nor has it been presented for defense in any other university program or for obtaining a different or the same academic degree.


   \section*{AI Transparency Note}
   I would like to acknowledge that I have used AI tools as part of the research and writing process for this thesis.
   I have ensured that all content generated by AI has been carefully reviewed, verified, and integrated into my work responsibly.
   I stand behind all the ideas, arguments, and conclusions presented in this thesis, and take full responsibility for its content.


   \newpage





   \thispagestyle{empty}

   \section*{Abstract}
   Most conversation research is done in a qualitative way.
   Recordings of human interaction are transcribed and research is being done on whatever is found in the data.
   Multiple instances of a certain phenomena in human interaction recordings can only be gathered by luck.
   Using human interaction recordings for
   studying rarer phenomena such as coreference disruptions is very hard.
   This paper attempts to tackle this challenge by using conversational AI.
   This way the course of the conversation can be influenced and
   programmed to contain the desired stimulus.
   Data elicited this way is however not guaranteed to be useful.
   Annotation needs to be done to confirm that intended stimulus has indeed taken place in the conversation.
   The aim of this paper is to determine whether such approach can be taken for disrupting coreference relations
   within the unraveling text of the conversation.
   This is done by having the chatbot respond using anaphora that is hardly or not at all assignable
   to any of the candidate entities previously mentioned in the text.
   Inter-annotator agreement has proven to be sufficient to annotate acquired data.
   The success in producing the various stimuli appears to depend on the stimulus complexity
   hinting at possible steps to improve the conversation design framework.
   The result of the quantitative observation was only partially as expected.

   \subsection*{keywords}
   conversation, coreference, coherence disruption, anaphora, chatbot, artificial intelligence




   \section*{Abstrakt}
   Výzkum konverzace se většinou provádí kvalitativně.
   Nahrávky lidských interakcí se přepisují a výzkum se provádí na základě toho, co se v datech objeví.
   Více případů určitého jevu v nahrávkách lidské interakce lze shromáždit pouze náhodou.
   Využití nahrávek lidské interakce pro
   zkoumání vzácnějších jevů, jako je například narušení koreference, je velmi obtížné.
   Tato práce se pokouší tento problém řešit pomocí konverzační umělé inteligence.
   Pomocí konverzačního programování  lze ovlivnit průběh konverzace tak,
   aby obsahoval požadované podněty.
   U takto získaných dat však není zaručena jejich užitečnost.
   Je třeba provést anotaci, aby se potvrdilo, že v konverzaci skutečně došlo k zamýšleným podnětům.
   Cílem této práce je zjistit, zda lze takový přístup využít pro narušování koreferenčních vztahů
   v rámci odkrývaného textu konverzace.
   To se děje tak, že chatbot reaguje pomocí anafory, kterou lze pouze těžko či vůbec přiřadit
   k některé z kandidátních entit, které se dosud objevily v textu.
   Shoda mezi anotátory se ukázala jako dostatečná pro anotaci získaných dat.
   Zdá se, že úspěšnost při vytváření různých podnětů závisí na složitosti podnětů
   což naznačuje možné kroky ke zlepšení rámce pro navrhování konverzace.
   Výsledek kvantitativního pozorování odpovídal očekávání pouze částečně.

   \subsection*{klíčová slova}
   konverzace, koreference, narušení koherence, anafora, chatbot, umělá inteligence


   \thispagestyle{empty}
\clearpage
\tableofcontents
\clearpage

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

\par
Recently, there has been a breakthrough in the way we interact with machines.\footcite{sharma2024exploring}
We can now instruct a computer using the natural language.
Besides making existing technology an extra step accessible,
new ways to use technology appear.
Being able to simply talk to a machine and have it respond
can help overcome longstanding challenges
such as notably accessing a knowledge base via semantic search.\footcite{makela2005survey}
Until recently a knowledge base would usually be accessed via fulltext,
meaning we would only be able to find
information where a part of its formal encoding is already known.
Today, we can search for information simply by asking questions,
including follow ups
all thanks to natural language computer interface\footcite{hendrix1982natural}.
\par
The promise of much practical usage of the current wave of generative AI is ambitious
and only brings its fruit slowly, perhaps slower, than was expected.
\footcite{bloomberg2024openai1, reuters2024openai}
There is talk of a "plateau" in development of the technology powering
the current cutting edge inventions.\footcite{ritter2024ai}
Besides that there are AI has issues like
high electricity consumption\footcite{ritchie2024ai} and
unpredictable and broad societal impact.\footcite{hagerty2019global,baldassarre2023social}

That being said, in the context of conversation research,
this development in technology promises to make things possible that previously were not.
It is possible to have partial control over what happens in the conversation and
have decent certainty, that
our system simulates human–human conversation to the user.
New kind of conversational data is in reach –
logs of the human–bot interaction, that could be categorized based on
which \textbf{researcher controlled stimulus} and
which \textbf{participant reaction} to given stimulus
they contain.
This approach can solve an issue conversation research has been facing since the beginning.

\par
In the 1960s the relatively recent emergence and adoption of telephone technology
allowed for recording and transcribing authentic conversational data.
This allowed for studying conversation in detail.
This advancement took place thanks to developement in technology
which is reminiscent of the current day situation.
While human–bot conversational data is arguably less authentic than telephone conversation transcripts,
experimental approach can be taken while the human element is present.

\par
The goal of this thesis is to provide a debate on whether
using generative AI is a viable methodology for conversation research.
This is done by attempting to develop that very methodology.
Proceeding we operate in a frontier,
the first steps should be establishing data-backed baseline knowledge
and assessing possible lines of research.

\par
Understanding what makes
the unraveling text of a conversation a coherent one
can be approached via obtaining and analyzing
conversational data containing coherence disruptions.
This can be done using the discussed technology,
because it has the capability of conversing in a way
that is found generally acceptable by humans
and can drift away from the coherent interaction
if appropriately instruced to do so.

\par
The data this thesis seeks to elicitate and analyse are
actual human–chatbot exchanges containing moments which
have the potential to be problematic for
the human participant to process and follow up on.
The boundary between what makes a conversation coherent or incoherent is blurred.
It is in no way a binary property of the text of the conversation.\footcite{givón2020coherence}

\par
While chatbots are evaluated for how natural and error free their way of conversing is,
human–human conversation is rarely flawless as
errors happen and
conversational coherence gets temporarily disrupted.
In case of human–human communication, disruptions can however be cured easily.\footcite{bublitz1999disturbed, DINGEMANSE202430}

\par
In human–bot communication, disruptions could derail a conversation completely,
leaving the bot, who would only rely on surface level textual clues, in the dark.\footcite{mctear2020conversational}
This has become rare with generative AI.
Even though it brings a set of its own problems like
frequently lacking factuality or
the difficulty to handle data responsibly,
the cutting edge technology powered conversation systems are

\begin{itemize}
\item
   better capable of understanding and producing relevant answers

\item
   able to return to their conversational point of departure
\end{itemize}

\par
Human–bot communication is often single–purpose.
Companies and institutions deploy voice and chat applications to interact with customers and clients,
so there is usually a goal to be achieved.
The coherence in each moment of such conversation can then be described based on whether
the goal is currently coming closer to being achieved with ease.
Another common frame for a human–bot interaction is an open–domain conversation,
also known as chit–chat or smalltalk.\footcite{ramnauth2024chitchatdevelopingrobotssmalltalk}
Since there is no global goal to achieve,
the coherence of such interaction is dictated by
a different set of factors\footcite{xu-etal-2021-discovering}

\par
Many factors that influence coherence in conversational texts,
both in human–human or human–bot exchanges,
have been studied:

%––
\par{\textbf{Politeness}
Brown and Levinson’s work on politeness strategies describes social alignment in smooth interactions.\footcite{brown1987politeness}
Politeness strategies, such as using polite language, offering options, or softening potentially face–threatening comments, help to create a comfortable communicative environment. These strategies align with social norms, which people interpret as markers of respect, consideration, or even trust. A failure to employ these politeness strategies, or using them inconsistently, can disrupt conversational coherence. For example, blunt or overly direct responses may be perceived as abrupt or rude, diverting the conversation's flow or causing discomfort. In such cases, the breakdown of polite norms can lead participants to question intent, hindering effective and smooth communication.


\par{\textbf{Speech acts}}
Following Austin and Searle's speech act theories, communication relies on expressing clear intentions and meanings that help build mutual understanding.\footcite{austin1962how,searle1969speech}
 When speakers convey intentions explicitly through statements, questions, requests, or assertions, it signals to listeners the purpose and direction of the conversation. Effective communication strategies help maintain coherence by ensuring each contribution builds logically on the last. On the other hand, unclear intentions or ambiguous phrasing can create misunderstandings, disrupting the conversation's flow. Misalignment or mixed signals – such as using sarcasm without cues or making indirect requests without context— can leave listeners uncertain about how to respond, leading to off–track or irrelevant contributions and possibly creating need to address the communication to regain understanding.

\par{\textbf{Conversational Maxims}}
Grice’s conversational maxims are fundamental to coherent dialogue.\footcite{grice1975logic}
They suggest that participants should:

\begin{itemize}
\item
provide truthful information (Quality)
\item
neither too much nor too little (Quantity)
\item
remain on–topic (Relevance)
\item
communicate in an orderly, clear manner (Manner)
\end{itemize}

These maxims encourage effective exchange by setting a standard for contributions that are informative, truthful, relevant, and unambiguous. When violated, such as by giving excessive detail, omitting important context, or straying from the topic, coherence suffers. For instance, irrelevant tangents or over–detailed explanations may confuse the listener as to what is the main focal point of conversation in that moment. This misalignment can leave participants uncertain about the conversation’s direction, ultimately diminishing coherence and the effectiveness of communication.

\par{\textbf{Sequence Structure}}
The work of Schegloff and Sacks on sequence structure and turn–taking emphasizes that ordered interactions support predictability and continuity in dialogue.\footcite{Schegloff_2007}
Turn–taking conventions — where participants follow an implicit sequence of speaking and responding — help maintain the flow by structuring conversation in a logical order. This sequence structure allows both parties to anticipate when to listen and when to speak, contributing to a well–paced, cohesive exchange. However, interruptions, abrupt changes in topic, or skipping expected responses can disrupt this sequence, introducing unpredictability that can confuse participants. These interruptions fragment coherence by shifting the conversation away from expected responses or structured flow, often leaving gaps in understanding or causing conversational breakdowns.

\par{\textbf{Message and Topic}}
Interactional linguistics underscores that consistency in message and topic preserves continuity in conversation.
\footcite{CouperKuhlenSelting2017}
When speakers stick to a shared topic or make gradual, clear shifts, coherence is
maintained because participants know what to expect.
Frequent or abrupt topic shifts, however, or sending unclear or conflicting messages, can create disjointed exchanges.
For instance, introducing a new topic without closure on the previous one can confuse listeners,
leading to a scattered or fragmented interaction.

\par
While all of the mentioned areas unveil much about the way conversation works,
they rarely concern themselves with the textual dimension of conversation.
Yet they significantly inform linguistics.
In the meantime
the lack of a true interpersonal dimension in human–chatbot communication
allows to focus primarily on the elements in conversational text
which make it cohesive and coherent or rather
those that have the potential to prevent it from making it so.

In this paper conversational data are collected
with the use of a chatbot
that contain various disruptions of conversational coherence.
The disruptions lie in coreference relations and consequently topical progression.
The chatbot is programmed to confront human participants with
different kinds of potentially coherence disrupting anaphora.
All the conversations are visited and filtered for those
that actually contain the intended anaphora stimulus.
Quantitative metrics are then used to verify that
the severity of the disruption of conversation coherence for individual stimulus corresponds to expectations.
Qualitative observations are made to assess
success of generating the stimuli and what next steps can be done to improve it.



% _____________________

\chapter{Theoretical foundations}


\section{Textual dimension of conversation}
\par
    This section outlines the key theoretical concepts and definitions
    that form the foundation of the present work.


\subsubsection{Text}
\par
    Text, in its broadest sense, refers to any form of communication that conveys meaning through a combination of signs, symbols, or language.\footnote{\cite[p.~7]{hrbacek1994}; see also \cite{hjelmslev2016}.}
    These semiotic structures can take various forms, including written, spoken, visual, or even non–verbal modes of expression.\footcite[p.~13]{barthes1977image}
    A text can be as simple as a single sentence or as complex as a novel, and it can exist across different mediums, from books and articles to advertisements and digital content.
    What defines a text is its ability to convey a coherent message or idea, often intended for interpretation by an audience or an adressee.
    Texts can serve a wide range of purposes, including storytelling, instruction, persuasion, or simply recording information.
    Typically text is a structure that is
    linguistic, produced and percieved as intentional and coherent.

\par
    The text of a conversation is specific because it is multi-producer.
    Another example of a multi-producer text
    would be a sequence of commercial signs on a busy street.
    It is the spatial juxtaposition of the signs and temporal juxtaposition of utterances,
    that make them a text.

    \par
    Another key property of conversation is that it is negotiated.\footcite{Sacks1992}
    This results from its inherently multi-producer and temporally dynamic nature.
    While some forms of written discourse—such as debates in newspaper columns or academic exchanges—also exhibit negotiation,
    these are relatively rare.
    Moreover, such written debates are better understood as a sequence of separate text units rather than a single, temporally co-present interaction.
    In contrast, conversation unfolds in real time, making its negotiated nature both more immediate and more structurally integrated.

\subsubsection{Coherence}
\par
    Coherence refers to the logical connections and consistent relationships that
    make a text easy to follow and possible to understand. \footnote{\cite[p.~83]{givón2020coherence}; see also \cite[p.~9]{hrbacek1994}.}
    It is achieved when the ideas, sentences, and paragraphs within a text are linked together in a meaningful way,
    allowing the reader to grasp the author's message without confusion.
    Coherence often depends on transitions from one section of the text to another,
    the logical flow of arguments, and the proper sequencing of information.
    It ensures that each part of the text contributes to the overall meaning, creating a unified whole.\footcite[p.~28]{hrbacek1994}
    Incoherent text can be difficult or impossible to understand, even if the individual sentences are grammatically correct.\footcite[p.~30]{hrbacek1994}
    It is a property of the whole text, but
    textual elements can be pointed out that contribute to or diminish the given texts coherence.
    Those elements are however not referred to as 'coherence elements'.

\par
    Coherence is a cognitive phenomenon\footcite{Roberts01101993} because
    it involves the mental processes of interpreting, organizing, and understanding information.
    When reading a text,
    coherence arises not only from the structure and linguistic cues provided by the author but
    also from the reader's ability to
    make connections between ideas based on prior knowledge, expectations, and context.
    This cognitive interaction between the text and the reader’s mind is what makes the content understandable.

\par
    In conversation, coherence becomes even more complex,
    as multiple participants are simultaneously contributing to and interpreting the flow of information.
    Each individual brings their own perspective and understanding to the interaction,
    which requires constant negotiation to maintain coherence.
    Misunderstandings, different backgrounds, and interruptions
    can disrupt the coherence of a conversation,
    making it a more dynamic and fragile process compared to single-producer text.
Whether a written text is coherent depends mostly on the reader\footcite{bublitz1999disturbed,Roberts01101993}
and whether a conversation text is coherent depends on\\ an ongoing negotiation


\par
    Coherence is a scalar property rather than a binary one.\footcite{givón2020coherence}
    It is also tricky to measure.
    this thesis seeks to explore one possible approach of
    declaring different levels of coherence disruptions
    and observing the acceptance rates in participants
    and corelation between them.

%–––
\subsubsection{Cohesion}
\par
    While coherence refers to the interpretative quality of a text,
    wherein the ideas form a logical and meaningful whole,
    cohesion focuses on the structural relations
    within a text, achieved through grammatical and lexical links.
    It should be seen as an umbrella term
    covering specific relations within
    the structure of the text,
    where cohesive elements can be directly pointed out.
    While coherent text does not necesarily need to be cohesive,
    cohesive elements often support it.
    A coherent text tends to be at least somewhat cohesive.

\par
    Halliday and Hasan\footcite{Halliday76cohesion} developed
    a detailed framework of cohesion, which includes
    endophoric references,
    relating parts of the text to each other, and
    exophoric references, which point outside the text.\footcite[p.~31]{Halliday76cohesion}
    Endophoric cohesion covers aspects
    like anaphoric references and cataphoric references.\footcite{hajicovasgall2003, loaiciga-etal-2022-anaphoric}
    Exophoric references, however, rely on shared context beyond the text itself,
    often requiring readers to use prior knowledge.

\subsubsection{Cataphora and Exophora}
    %–––
\par
    In Halliday and Hasan's framework,
    cohesion in language is achieved through various devices that connect
    different parts of a text, forming a unified whole.
    They classify cohesive ties as
    references, substitutive forms, ellipsis, and connectors, with
    anaphoric references being one of the primary ways texts achieve cohesion.\footcite[p.~68]{Halliday76cohesion}
    When a text element cannot be mapped to a preceding referent,
    Halliday and Hasan suggest that cohesion is maintained through
    shared situational understanding, making the reference exophoric.
    Cataphoric references, though less common, involve
    elements that look forward in the text,
    showing intentionality by the author but contributing to
    cohesion primarily through the eventual resolution of the forward–pointing referent.

\par
    In a conversation if a seemingly anaphoric text element is not succesfully mapped
    to an antecedent
    the reference can still be understood, because of shared context.
    Such element reaches out of the text with its reference, making it an exophoric one.
    Cataphora is a related phenomenon –
    a reference which points forward in the text.
    Such occurence is relatively rare in written text and even more so in conversation.
    In fact it is somehow pointless to account for cataphora in a multi–producer text.
    A cataphora denotes an authors intention to reveal
    the nature of a referent explicitly after first mentioning them.
    In conversation, where multiple contributors cocreate given text,
    and mutual understanding and agreement is the measure of
    how coherent the produced text is, later realisation of a vague reference
    does not contribute to how coherent it is.
    Regardless, in case of a cataphora,
    only the referent is a cohesive element,
    not the cataphora,
    as it ties back to the previous text, creating bonds across large textual units.

\subsubsection{Anaphora, Endophora and Coreference}
\par
    A common cohesive means is anaphora.\footcite{Nedoluzhko2011}
        An anaphoric element is by definition endophoric.
        It points inside the text it appears in.
        An anaphoric element refers back in the text to its antecedent, with which it is coreferent.
    In Czech, anaphoric references often rely on grammatical gender and number,
    making participial endings essential for identifying the referent.
    For instance, when a gramatically masculine entity is mentioned,
    later references might use a participle in the masculine form, such as šel ("he went"),
    connecting back to it without repeating the noun or using a demonstrative.
    Demonstratives, such as ten ("that") or tento ("this one"),
    also frequently serve anaphoric functions, guiding the reader to a previously mentioned subject.
    Temporal and locative adverbs, such as tam ("there") and tehdy ("then"),
    also contribute cohesion by indirectly referencing time and place details introduced earlier in the text.
    These anaphoric elements strengthen textual coherence by reducing redundancy.
    The reader identifies coreferential links through these markers,
    following the cohesive threads without needing explicit repetitions.

\par
    In conversation, many aspects of which are subject to negotiation,
    also specific coreference relations can be questioned.\footcite{loaiciga2021reference}
    The reference realised by one communication participant may be unclear to the other
    resulting in a repair request coming from another particiapant.
    In conversation analysis,
    %–––
    Sacks’s concept of repair traditionally addresses
    misunderstandings related to intentions and actions,
    loosely drawing on frameworks like Austin’s and Searle’s speech act theories.
    From this perspective, repairs\footcite{DINGEMANSE202430} often target interpretative gaps about
    what a speaker intends to do with their utterance.
    However, viewed from a broader perspective,
    what is called repair triggers can extend beyond intentions alone,
    encompassing issues on the textual level as well.
    For instance, an non-assignable anaphora —
    a reference that lacks a clear antecedent —
    may lead to a repair request,
    thereby showing how textual ambiguities prompt interactional responses.
    This approach expands the causes of repair in conversation,
    integrating elements of reference and interactional misalignment,
    where a structural aspect of the language itself can
    become a repairable issue in the communicative exchange.

\subsubsection{Topic}
\par
    Topic is what a text or its portion is about.
    Topic is complicated to annotate.
    Among others, some issues with topic and annotating it in text are:

\begin{itemize}
    \item
    A text can and typically does cover multiple topics simultaneously
    \item
    Different framing produces different topic annotations
    \item
    The span of a topic section can be impossible to delimit within text.
    \item
    Topic annotation generates more text, which can also be annotated for topic.
\end{itemize}

\par
    Despite all these complications,
    topic cannot be skipped in conversation research
    as it is deeply intertwined with the concepts mentioned above.
    Topic progressions across text are realised via anaphora and association
    and tightly interact with coherence.
    An sufficient amount of time has to be spent on a given topic unit,
    enough information has to be said about a given topic
    in order for it to be possible to move on or add another one in the conversation.
    Closure has to be provided in order for a topic to be done.
    Transitioning from one topic to another has a potential to disrupt coherence,
    if the association between the topics is too distant.
    A divergence in topic has to be justified.

\subsubsection{Association}
\par
    Association is a realization of an isotopic relation.\footcite{koblizek2015}
    Lexical elements in text exist in a semantic web of relationships.
    Similarly to coherence, associative relationships are cognitive phenomena.
    They come to exist when they are percieved.
    While association is a cohesive element it is difficult to formalize
    the way it can and has been done with anaphoric text relations.
    It is however a major factor in a coherence of text as
    in some cases a text can only rely on association in its coherence.

\subsubsection{Entity}
\par
    An entity is a text element that can be referred to by an anaphora.
    It is usually a lexical description of an object, person, event etc.\footcite{entities}
    Words or text elements which can be reffered to by an anaphora will be called entities.
    Since a phrase containing an anaphora typically adds more information about the referent
    the new information must be semantically compatible,
    in other words association has to be possible between the referent and the added information.
    Entity also has to do with topic.
    In text topic can be represented by a single or multiple entities.
    Coreferent words will be regarded as a single entity.
    It can serve to partially map a topic distance in the texts chronology.

\section{Interactional dimension of conversation}
\par
A conversation text is produced by multiple producers.
Conversation is an interactive process, distinct from static text, which is created collaboratively.
Conversational content is continuously negotiated by participants, who continuously adapt to one another.
Due to its temporal and cooperative nature, conversation allows for:
\begin{itemize}

    \item
    Overlaps in speech,

    \item
    Swift corrections of minor errors,

    \item
    Multiple layers of perspective, including:
        \begin{itemize}

            \item
            Each participant’s personal viewpoint,

            \item
            Each participant’s perception of others’ viewpoints,

            \item
            Each participant’s understanding of the shared conversation as it’s being co–created.
        \end{itemize}
\end{itemize}

        Each of these perspectives can become desynchronized, leading to misunderstandings.
        However, humans are excellent at resolving misunderstandings,
        because under normal circumstances, people tend to cooperate.

\par
The Cooperation Principle, introduced by H.P. Grice,
suggests that participants in a conversation typically work together to achieve effective communication.
Grice proposed that, to ensure this cooperation, speakers follow four conversational maxims.
In practice, people may not always follow these maxims
but they do so in ways that still rely on shared expectations of cooperation.
Even when misunderstandings arise,
humans naturally engage in conversational repair,
using their social intuition and mutual cooperativity to clarify intetion and realign perspectives.

\par
Modern conversation research traces its roots to conversation analysis,
a field pioneered by sociologists Harvey Sacks and Emanuel Schegloff in the 1960s.
They sought to understand the structure and
social rules of everyday interactions,
focusing on the patterns and norms that govern turn–taking and response.
Thanks to recordings of phonecalls,
transcripts could be qualitatively analyzed.
This research has lead to coining new terminology.

    \subsubsection{Adjacency pair}
    \par
    Adjacency pairs are sequences of two related utterances by different speakers.\footcite[p.~188]{Sacks1992}
    These pairs are characterized by their predictable and reciprocal nature, where the first part sets up the expectation for a specific type of response. Common examples include greetings('Hi' → 'Hello'), questions and answers ('What time is it?' → '3 PM'), or offers and acceptances/declines ('Would you like some coffee?' → 'Yes, please' or 'No, thank you').

    \subsubsection{Sequence structure}
    \par
    Sequence structure refers to the organization of conversational turns into coherent patterns or sequences. It describes how interactions are shaped by predictable structures, such as adjacency pairs. These sequences provide order and meaning to conversations, guiding participants in understanding when and how to respond. Schegloff\footcite{Schegloff1990} emphasized that sequence structure is central to the social organization of talk, as it allows participants to manage and negotiate interaction effectively.

    \subsubsection{Topic shading}
    \par
    Topic shading, as discussed by Sacks\footcite{topicshading}, refers to the subtle way in which a conversation naturally shifts from one topic to another while maintaining coherence. Instead of abruptly changing the subject, speakers introduce a related idea or concept, gradually steering the discussion in a new direction. This process allows for smooth transitions in dialogue, helping participants maintain engagement and avoid confusion.

    \subsubsection{Dis/preferred answers}
    \par
    Preferred answers, according to Sacks\footcite[p.~410]{Sacks1992}, are responses in conversations that align with social norms and expectations, making interactions smoother and more cooperative. In conversation analysis, preferred answers typically follow the format or intent of the preceding question or statement . They contrast with "dispreferred" answers, which might include refusals or disagreements and often require additional explanation or mitigation to maintain social harmony.

    \subsubsection{Conversational repair}
    \par
    Conversational repairs refer to how participants address and resolve problems in understanding, hearing, or speaking during interactions\footcite{sacksRepair}
    These issues, can occur at any point in a conversation. Repairs are classified into self–repair, where the speaker corrects their own error, and other–repair, where a different participant addresses the issue. They can further be classified into a self–initiated repair and an other–initiated repair.

\par
%–––
As a descendant of conversation analysis interaction linguistics has emerged,
building on its insights to examine language use in social contexts.
It broadens the focus to study not only verbal exchanges but also
multimodal cues like gestures, gaze, and intonation,
analyzing how these elements contribute to meaning.
Interaction linguistics aims to understand the dynamic aspects of conversations,
such as how topics shift and how sequences of speech acts unfold,
reflecting the fluid nature of human communication.

\section{Disruptions in conversation coherence}
\par
While the question of what makes for a coherent text is too broad,
the answer to what makes for a coherent conversation can be somewhat easier to answer.
Because conversation participants negotiate understanding,
it is up to them, when a conversation is and is not coherent
to describe what a coherent conversation is,
it is worth pursuing the moments, when the conversation stops flowing with ease.
Such moments can be called coherence disruptions.
A coherence disruption is a complex phenomenon as

\begin{itemize}
\item
it penetrates through some or all of mentioned perspectives on an on–going conversation
\item
it can't be evaluated in a binary fashion
\end{itemize}

There are different ways a conversation coherence can be disrupted:
\begin{itemize}

\item
   if a participant suddenly starts speaking
    in a way that can hardly be considered interaction
    due to its irrelevance or

\item
   if the utterance simply is not grammatical or understandable,
    while the conversation has been compromised and becomes incoherent,
    it has more to do with incoherent written text, because
    the incoherence is encapsulated on the level of a single utterance
\end{itemize}

This thesis focuses on neither of these.

Roberts\footcite{Roberts01101993} discusses various types of incoherent text.
He exemplifies so called giberish as incoherent text that is lacks structural relations.
On the other hand he discusses experimental theater or literature as a type of text
which is assumed to be coherent in the sense that there is an intention behind it
but contains little to no structural relations.
Lastly he mentions a so called "schizophrenic discourse" as a speech that
is not assumed to be coherent even if it contains structural relations.
In any case Roberts definitively states that coherence is assumed
and is therefore a receptive phenomenon.
The incoherence Roberts discusses is considerably different from when
the source of incoherence stems from
    the structure of the conversational text or
    relationship between different utterances
    – this is when another participant assesses,
        they are simply speaking leading a different conversation
        perhaps with a differing intention
        or that they are conversing under differing set of circumstances
        which manifests formally in the linguistic fabric of the conversation – its text.
        All that despite everyone included being cooperative.

\subsection{Sources of incoherence in conversation}
\par
%–––
Schegloff shows how incoherence arises when
people interpret sequence structure differently,
namely in terms of which turn is seen as an answer to which previously occuring turn.
In his example, the participants misread each other’s intentions,
leading to confusion about how their turns fit together.
Each of them projects different expectations for how the conversation should unfold,
which causes misaligned sequence structure interpretations.
When this happens,
they turn to brief metacommunication — comments about the conversation itself
to try to clarify and re–align their understanding.
Schegloff illustrates how these
efforts to "repair" the misalignment are central to
managing and resolving incoherent moments in conversation.

\par
Coherence disruptions are also discussed in literature.
%–––
Hrbáček’s approach to coherence and cohesion in text distinguishes the two concepts,
noting how they often interact but can also be independent.
He highlights that while
cohesion involves grammatical or lexical links that
make sentences flow together,
coherence relies on the logical and meaningful progression of ideas.
This means that a text could be cohesive –
using connectives, repetitions, and consistent lexical choices –
yet lack coherence if the sequence of ideas doesn’t
make logical sense or follow a clear progression.
Conversely, a text may be coherent in its narrative flow without
relying heavily on cohesive devices.
In Czech linguistics,
the distinction between téma (theme) and réma (rheme), as used by Daneš,
underlines the role of topic progression.
Hrbáček illustrates this by discussing examples where
a story progresses logically from one point to the next while
being incoherent despite being clear about its topic structure
due to never coming back to a previously mentioned topic.

\par
Two kinds of phenomena are at hand when it comes to
ways in which conversation coherence can be disrupted –
topic shifts and non-assignable anaphora\footcite{bublitz1999disturbed}
While not unique to conversation
both take on specific forms in it worth looking at.

\subsubsection{Topic shifts}
\par
    When conversations shift abruptly from one topic to another,
    it can create confusion for the conversation partner.
    They might find themselves trying to
    reconnect to the previous discussion or
    wondering how the new subject relates.
    This can lead to misunderstandings
    as the transition can feel jarring.

\par
    One interesting question is,
    how do we determine when a topic has run its course?
    What common traits do conversations share when a subject is truly exhausted?
    Perhaps observing transcripts could reveal repeating patterns in topic progression or sequence structure.

\par
    Moreover, what makes for a smooth transition between topics?
    Is it related to the cues participants give each other,
    or perhaps the context of the discussion?
    How do we navigate the flow of conversation and
    what indicates a natural shift versus a disruptive one?

\subsubsection{Non-assignable anaphora}
\par
    non-assignable anaphora is closely tied to topic progression.
    Currently established topic or topics help assigning anaphora and
    determining between an anaphora and an exophora.
    Even if an anaphoric device is not assignable,
    and the reference is presumably an exophoric one,
    The reason for employing this reference must be
    relevant to an established topic.
    In conversation meaning of demonstratives is to be negotiated.
    If an anaphoras assignability causes confusion,
    chances are it is caused by one of the following

\begin{itemize}
\item
there are no relevant assignment candidates

    \quad
    this situation can be understood as a vague or unjustified exophora

\item
there are multiple equally relevant candidates

\item
candidate has occured in the conversation text too long ago

    \quad
    can be understood as an abrupt return to previously established topic
\end{itemize}

%–––
\subsection{What do people do about coherence disruptions?}
\par
In conversation, coherence disruptions often prompt participants to
employ strategies to maintain understanding and flow.
Schegloff suggests that people manage these disruptions through interactive repair or inference.
Interactive repair often involves
explicitly addressing misunderstandings or clarifying intentions,
often by rephrasing or asking questions.
Interactive repair refers to immediate, collaborative corrections within dialogue,
where one speaker might correct the other or themselves to enhance clarity.
Inference and pragmatic reasoning, the most seamless methods,
allow participants to fill gaps based on context and social cues,
helping conversations continue smoothly without explicit repair.

\par
Dingemanse and Enfield\footcite{DINGEMANSE202430} echoes this from a cognitive perspective,
highlighting how inference and pragmatic reasoning are particularly effective.
Participants rely on shared understanding and contextual knowledge to interpret ambiguous statements.
Together, they use both
explicit (metacommunication and repair) and
implicit (inference and reasoning)
methods to restore coherence.

\par
It needs to be noted however that both interactive repair and reasoning are
deployed in a number of other contexts
other than conversation coherence disruption.
Inference takes place constantly\footcite{garfinkelstudies}
Each of those moments could be hardly considered a coherence disruption.
There is however always potential for it,
particularily via unclear or non-assignable anaphora or abrupt unjustified topic shifts.
Repair and metacommunication also takes place in a mutually informed and synchonized interaction.
It is for example deployed when it is revealed
that the interaction participants intentions or opinions differ.

\par
These uses of interaction management are however
hardly possible to analyse on a textual level
since they do not cooccur with coherence disruptions.
What can be observed are – as mentioned above –
troublesome anaforic references and topic progressions.


% _____________________
%

\chapter{Experimental framework}

\section{What are chatbots?}
\par
A chatbot is a conversation simulation application.
An attempt to make a machine converse with a human user requires mimicking human speech.
The intention is to have a user interact with a chatbot
that would communicate so well that
the user would be convinced
this is another human they are talking to.
Whether that has been achieved would be measured by a so-called Turing test
proposed by Alan Turing in 1950.\footcite{turing1950computing}

\par
Initial attempts at making a computer converse were rule–based.\footcite[p.~43]{mctear2020conversational}
The content of the chatbot's utterances
would be predetermined, and there would be a decision tree that would decide what to say next.
In the early days, as well as oftentimes in modern-day systems,
string matching\footcite{stringmatching} would be used to analyse user input.

\par
The chatbot ELIZA\footcite{weizenbaum1966eliza} is regarded as a milestone in conversation AI.
It pretended to be a therapist
using general phrases and questions.
This led to a relatively believable conversation.

As long as the interaction frame is strictly defined and
the robot has some level of authority
like in the case of ELIZA
the rule–based approach can work.
In modern days, this principle is still used in large enterprise systems.
In practice, this approach works as long as there is surveillance and maintenance.\footcite{kolosova2022}

\par
Machine learning moved the possibilities of conversational AI forward.
Multidimensional semantic space-based classifiers like the ones used in IBM Watson\footcite{building_watson_2010}
allow for understanding declared categories while keeping the structure rule-based.

\par
The recent breakthrough of large language models using the transformer architecture
seemingly solved the conversational AI problem altogether.
It is now possible to generate near-natural speech.
This gives the possibility to let the conversation be taken over by one answer generator.

Chatbot implementations of large language models are very useful for some use cases
like open domain conversation or accessing a knowledge base.
Moreover, in 2024, this technology is now closer to beating the Turing test than
any other model or approach before it\footcite{jones2024peopledistinguishgpt4human}
by having 54\% of participants thinking
they are talking to a human.
While Eliza convinced 22\% of participants,
actual humans only convince 67\% of participants.

\par The downside at hand is, however loss of control.
To give a dialog system the power to make changes in adjacent systems,
its behavior needs to be programmable.

\subsubsection{Turn taking in chatbot interactions}

\par
Even if the Turing test is passed,
a true simulation of conversation
can only be achieved if the temporal aspect of conversation
is simulated.\footcite{optimizing-turn-taking}

\par
As established in the previous chapter, turn-taking is a crucial aspect of conversation.
The way participants determine who is to talk
explains the difference between
a structure of the text of a conversation
and a single–producer text.
The mechanism of turn-taking differs
between actual human conversation
and an interaction between a chatbot and a user.

Interaction between the chatbot and the user
typically take place in a strict fashion
where both participants,
human and virtual,
have unlimited time to come up with the next answer.
While the chatbot should be optimized to answer as fast as possible,
the user has as much time as they need until a technical fallback.

\par
\say{Research in sociolinguistics, psycholinguistics, and
conversational analysis has revealed that
turn–taking is a mixed–initiative,
locally coordinated process, in which
a variety of verbal and nonverbal cues such as
eye gaze,
body pose,
head movements,
hand gestures,
intonation,
hesitations, and
filled pauses
play a very important role.
We continuously produce and monitor each other for
these signals and can coordinate seamlessly
at the scale of hundreds of milliseconds
across these different channels
with multiple actors.}\footcite{turntaking}

\par
People are capable of producing and picking up clues
that indicate opportunities for turn-taking easily.
Research is going
into figuring out the correct time to start speech\footcite{turntakingreview,GRAVANO2011601}
or creating a system that can produce such behavior.\footcite{distributedturntaking,Gervits2020Sigdial}
This research field
has the potential to push conversation technology
closer to true conversation simulation.

\par
A truly flexible turn-taking has not, however, been attempted in this thesis.
Data consisting of conversation transcripts with no overlap
is helpful for the textual analysis that is conducted here.

\section{Convform}
\par
An exploration has been carried out using a custom tool called Convform.\footcite{convform}

At its core, Convform is a computer program
which accepts a configuration, user input, and context
and determines the next chatbot answer.
Other than that, it offers a collection of utilities
to help design and run chatbots.

\subsubsection{Participant facing chat interface}
To handle the inputs, Convform provides a chatting environment
for the participants to interact with a chatbot.
The Convform environment differs from a usual chat log
because it does not display
the entire history of the conversation.
In an attempt to simulate spoken conversation,
it only displays the last chatbot response.
This way, the participant has to rely on their memory
in taking part in the conversation
like they would in spoken interaction.
Other than that, the participant may enter their next response
and send it.
They are also instructed to end to conversation by a red button
if the chatbot behaviour is "unnatural" (nepřirozené)
After the conversation is over, whether it has been ended by the user or the chatbot,
there is a questionnaire which
asks the participants to rate how "natural" the conversation was
and mark and comment on utterances in the now fully displayed conversation.

\subsubsection{Conversation design tool}
Convform lets admin users create chatbots and define their behavior.
The behavior can be defined by string-matching rules or prompts.
It is capable of working as a state machine or a stateless chatbot.
It provides a level of control over references within the design.

\subsubsection{Testing and debugging of various conversation contexts}
While designing chatbots, it is necessary
to be able to simulate various situations
to fine-tune various possible scenarios
that might occur in the conversation.
To achieve this, there must be a way
to encode the required context to Convform.
Convform chatbots use a custom conversation status object
to represent their current understanding of the conversation.
It contains information about the history of the conversation, which, in conjunction with the configuration file and user input,
helps determine the next response.
The configuration file is static, and
the conversation status is updated automatically.
User input comes from the user.
This conversation status can simulate any possible conversation context
from the chatbot's perspective.
For testing and debugging specific contexts, Conform allows the admin user
to tweak the conversation status.

\subsubsection{Accesing the conversation data}
Lastly, Convform naturally includes a convenient way to read user interactions
and browse associated conversation status objects

\section{Conversation design in theory}

% https://ieeexplore.ieee.org/abstract/document/9447005

\par
Designing the behavior of a dialog system
is referred to as conversation design.\footcite{kolosova2022,mctear2020conversational,cxd}
It is not the course of any one conversation that is being designed here
but rather as many possible ways any conversation could go
for a given use case.
Conversation design as a profession is deeply connected
with the rule–based approach that has been used since ELIZA times.
A conversation designer's task is typically maintaining all the possible utterances and
rules under which they would be uttered
in enterprise dialog systems.

A conversation designer operates between
the business logic and use case of the dialog system
the clients, customers, or users interacting with the system
and the developers maintaining the system.

\subsection{Rule–based approach}

To be able to design a rule–based dialog system,
one needs to be able to encode the following:

    \begin{itemize}

        \item
        The possible utterances that the dialog system can produce

        \item
        Rules under which the next utterance will be chosen
    \end{itemize}

\par
If the conversation is supposed to be a state machine, e.g.
it needs to be able to use different sets of rules
under different contexts in the conversation.
This way, a dialog system can be context-aware to a degree.
A conversation design of this sort
can be displayed as a diagram.
Then a way to maintain the context of the conversation is also necessary.
This context needs to encode rules to choose an immediate ruleset
that helps determine the next utterance.
This principle is a simplification of
how people decide what they will say next in conversation.

\subsubsection{Pros and cons}

\par
This approach to designing a dialog system
has been the standard for decades.
It offers granular control over how a dialog system should behave.
In case of the state machine variant, it
allows to guide the user through a relatively complex process.
It, however, suffers from how unpredictable the user can be.
It is up to the conversation designer to cover all the possible ways of answering
which is hardly possible
but also poses a necessity to segment the spectrum of possible answers
which can generate conflict when
a user input semantically spans across multiple determined categories.
This issue is even stronger while using the string matching approach,
because there the string literal can decide about the following dialog system answer
as if meanings and their speech representations would map one–to–one,
which they are not.
Even if a certain meaning is included in a ruleset,
the system might not grasp the meaning and react incoherently.
With the state machine, the
distribution of various rules across various rule sets
requires a big effort.
Extending the capabilities of a rule–based dialog system
hardly scale and tend to have regressions.
In case of dialog systems relying on user input by speech transcription
the text input processed by the system is not guaranteed to represent
what the user said.
In conclusion,  a rule-based approach to conversation design
provides control over the dialog system behavior
but tends to be inflexible and unreliable.

\subsection{Statistically driven approach}

Some of the issues tied to rule–based systems
are resolved, incorporating machine learning techniques into the dialog management of the dialog system.
As mentioned above, the recent breakthroughs in the field
of speech generation has been significant
allowing for letting the dialog system to play a bigger role
in what is being said next.
In its simplest form,
it is possible to just let the answer be generated "end–to–end".
The user input is sent to a model, which generates an answer.
While not perfect\footcite{tie2024llms}
this technology is capable of staying on topic\footcite{sreedhar2024canttalkaboutthisaligninglanguagemodels},
mirroring\footcite{ivey2024real}
and other things that make for a coherent conversation.

\subsubsection{Large language models}

The main component that is responsible for
this way of simulating conversation at this level of flexibility
are so-called large language models\footcite{naveed2023comprehensive}
These models, powered by advanced neural networks,
have revolutionized the field of natural language processing.
Among the most influential architectures are transformers,
which enables these models to handle
vast amounts of text data and capture complex patterns of meaning, context, and grammar.
In simple terms, they use their training data to generate the next most probable token.

These systems are trained on immense datasets,
allowing them to generate coherent and contextually relevant responses across various topics.
This flexibility has made them increasingly mainstream,
being integrated into tools for writing, education, customer service, and more.

\subsubsection{Prompt engineering techniques}

The rapid advancement of LLM technology
has outpaced research into optimal interaction strategies.
Understanding how to engage effectively
with these systems has been a developing area\footcite{sahoo2024systematic},
which illustrates both their power and their novel nature.
The foundational idea is:
an LLM performance improves significantly if it is prompted correctly.

Over time, researchers and practitioners have developed techniques for
crafting effective prompts to optimize outputs.
The simplest approach is known as "zero–shot" prompting,
where a user poses a direct question or request without additional context or examples.\footcite{kong2023better,li2023practical}
However, zero–shot prompting may not always yield the desired depth or accuracy.
It is common for the model to "misunderstand" the assignment
and generate tokens so that it will "confuse" itself
and lead to a generated answer in a completely irrelevant direction.\footcite{hwang2025llms}

More sophisticated strategies include "few–shot" prompting\footcite{yao2023more},
where examples are provided to guide the model's response style or focus.
This way, there is a reference for the structure of the answer, and there is a protection to the answer leading somewhere it is not meant to.
Framing the task as an analogy can help improve the output.\footcite{stevenson2024can, qin2024relevant}

Another very prevalent way that has proven to
improve the performance of LLMs is a so-called
chain–of–thought prompting.\footcite{wei2022chain}
It encourages the model to
articulate its reasoning step–by–step,
enhancing logical accuracy.
There are many ways to achieve this,
but the primary one is a few–shot approach
where a description of the logic is
explicitly described.
The model is then prompted to produce
a similar chain of thought
and end the answer with the sought-after information.

This principle can be further improved by
chaining several LLM calls and having one
evaluate the previous one.
Such a strategy has proven to surpass other models
in available metrics.\footcite{wu2024comparative}

\subsubsection{Pros and cons}

Using large language models as a core component
of dialog systems brings resolution to many issues
rule–based systems are introduced.

An LLM-powered dialog system is flexible in understanding
the user input.
The user input is processed in a much more sophisticated
than the string–matching or approach.
While the classifier approach is a lot more capable of understanding,
it is still forced to choose a predefined answer, whereas
an LLM can tailor an answer for every input.
It can do this in a way that would be very hard to come up with
especially in advance with the help of a conversation designer,
leveraging the fact that LLM is primarily a text generator
and only functions as a component in a dialog system.
It can be relatively well controlled as
it can accept complex instructions as to how to behave
and these instructions can be tuned at runtime.

Systems of this sort, however, introduce their own set of problems.
A big issue with factuality is called hallucinating.\footcite{bruno2023insights, perkovic2024hallucinations}
Factuality is a challenge for LLMs overall.
It is possible to ask questions
that do not have a correct answer.\footcite{payandeh2023susceptible}
It has been shown that LLMs have an issue
knowing that they do not know something.\footcite{yin2023large}
Recognizing that is the case requires
an extra level of reasoning
that is an object of research as of recently\footcite{transformer_circuits_2024}
They are also required to be ethically aligned with humans, which is an ongoing research.\footcite{wang2024comprehensive}

Even if all the programming and training are done for the benefit of humans,
information technology is susceptible to being broken by malicious action
or so-called jail breaks.\footcite{zhou2024don, wei2023jailbroken}
LLMs are being trained on a vast amount of data,
they hold knowledge that can be illegal or unethical to spread
like steps to create explosives, for example.
The typical examples of jail breaks are ways to manipulate
the LLM to give out this information
which, under regular circumstances, it would not give.

This is why for dialog systems that are supposed to achieve
anything else on top of the conversation itself,
if they are meant to be powered by LLMs,
a regulating structure needs to be placed on top of the LLM.

\section{Conversation design in practice}

\par
Conversation design in Convform attempts
to combine elements of rule–based design with text generation.
It allows creating purely rule–based chatbots
which analyse the user input based on string matching
and say exactly what they are prescribed to.
On the other hand, it also allows us to make the chatbot
understand the user input by adding it to a prompt
and answer using a generated response.
Both these approaches can be combined in various ways.
Other than that, Convform also allows to predetermine
the chatbot's personality for the entire conversation.
The building blocks of a Convform chatbot are
states and intents, which represent the utterance and the understanding.

\subsubsection{State}
A state is an object that carries several pieces of information
bundled together.
At its core, it contains the utterance of the chatbot
whether it is a hardcoded one or a prompt component that is to be called.
A state, however, also contains information about
which intents to listen to in the next user input,
which states to add automatically to the next response
and other navigation instructions like this one.
Each Convform status is associated with a response
can contain multiple states.
This is to make Convform generate more complex answers
which can react flexibly.
However, it also comes with a challenge to order these responses correctly
and make sure that they are not contradicting each other, content-wise.

\subsubsection{Intent}
An intent is an object representing a category of a user response.
It contains the information to determine whether
user input fits in the given category and
the state or states to respond with next.
Just like in state, the information about whether the user input
corresponds with the intent can be encoded via
string–matching patterns or a prompt.
As mentioned, intent is a problematic concept,
because it forces an outside logic and categorization
on user input, which might not be able to fit well
in the framework declared by the current intent set.
It is, however, the only way for a conversation designer
to peek into what is going on in the conversation
and to direct the dialog system correctly.

\par
Using a combination of states and intents, a Convform chatbot can be created,
that are instructed to lead from one state to another
make decisions based on intents
while being able to use any combination of
hardcoded responses and intent patterns
and descriptions of responses or user inputs used in intents.
A detailed description of how Convform works can be found
in the wiki of its GitHub repository\footnote{https://github.com/almarsk/convform/wiki}

\subsubsection{Coherence}
With the support of LLM-powered responses
Convform can be used to simulate
an open-domain–domain conversation with a user
and simultaneously using a combination
of intents and prompting
a Convform chatbot can be created
that acts incoherently under a predefined set of conditions
allowing the creation of experimental stimuli.
First, however, regular conversation needs to be achieved using Convform.

\subsubsection{Conversation style}

To simulate conversation, it is useful to simulate a persona.
The persona can then have a simulated motive to converse, which can interest the user enough to engage in interaction with the dialog system.
For rule–based systems, persona can be defined ahead of time, and it can manifest itself via the specific writing of the hardcoded responses
that the system is able to give.
With generated responses, the persona of the chatbot has to be included in the prompt.
The personality of LLMs and conversation technology more broadly
is being discussed.\footcite{gpttoxicity, robopersona}
The general characteristics of a machine talking to a human are typically
friendliness and helpfulness.
For conversation research with Convform,
the goal is to achieve just that.
The chatbot persona needs to be friendly,
polite and curious.
It needs to be able to keep the conversation going
but not change topic too often.
It needs to be able to add a little bit of its perspective.
\par
The conversation style, e.g., the amount of participation and initiativity in conversation
is something people adapt into their conversation counterparts.
Since developing a system that would imitate this behavior
requires additional effort
and expands the scope beyond the coherence research
this thesis focuses on,
this approach to conversation design
has not been taken here.
Instead, two versions of conversation style
have been developed
and distributed evenly between participants.

\par
The initial conversation style used in the experiments
represents a curious and friendly chatbot
which is instructed via a prompt to ask lots of follow-up questions.
This tends to result in a conversation that moves forward in its topical structure
in way deemed incoherent by Hrbáček\footcite[p.~30]{hrbacek1994}
It depends on the participant's impression whether it would be perceived as
curious and initiative or shallow and dismissive.

\par
A second version of the conversation style has been introduced
to get some insights on participants' acceptance and
the course of conversation itself.
This one would interleave topical questions with remarks on the topic
The intention behind this would be to slow down the conversation tempo
and allow the participant to bring their initiative.

\subsubsection{Prompting}

\paragraph{Entity recognition}

A few-shot prompts were deployed to track entities that could be referred to, which would help
keep track of which entities have been mentioned.\footcite{Loiciga2022NewOO}
Since GPT4o, the model used in the experiment,
tended to consider too many things as an entity,
most examples are negative and do not capture an entity.
It also contains some repetition as a result of fine–tuning the best wording.

\paragraph{Anaphorization}

To be able to create conversation designs that contain various types of anaphora,
convform first needs to be able to give a response that
has an anaphoric reference to an entity from the previous conversation.

The GPT-4o model used for this use case
does not tend to generate sentences with anaphoras.
Instead, it mirrors the entity phrase.

The anaphorization prompt, therefore, tasks the model
to modify a generated response so
that the mirrored entity is replaced with
An anaphoric device.
For this, another few–shot prompt was used to modify a just-generated response
found in Appendix under \textit{anaphorization}.
The approach here is the opposite of automated annotation of anaphora in text\footcite{loaiciga-etal-2022-anaphoric}
Instead, we are generating it.
This prompt has been tuned to catch as many tricky cases as possible.

\subsection{Stimuli}

With these tools
multiple chatbots have been created
that would generate conversation situations
which serve the role of experimental stimuli.
Participant reactions to these stimuli
can be then compared.
This way, conversational experimental designs can be created.
There are three types of stimuli created
for this thesis.
They are shallow anaphora, deep anaphora, and non-assignable anaphora.

\subsubsection{Shallow anaphora}

A shallow anaphora is a kind of anaphora
where the referent of the anaphoric device
should be relatively easy to map
as opposed to a deep anaphora.
The referent always occurs
in the preceding utterance of the participant.
This type of stimulus is relatively simple to achieve
in convform
generating a response
and using the anaphorization prompt on it afterwards.

Shallow or surface anaphora\footcite{hoji2003surface} is common in regular conversation
and should not pose a problem for the participant to understand
in a conversation with a chatbot.
It should therefore not have an impact
on the user acceptance of the chatbot
and should generally go unnoticed.
It is worth using as a stimulus
for a reference, an unproblematic case
that still requires the same kind of processing
as other more interesting stimuli.

\begin{quote}
\begin{itemize}[label={}, leftmargin=0pt, itemsep=0.5em]
\item Example:
\item \utterance{Participant}{I love coffee}{}
\item \utterance{Chatbot (not anaphorized)}{What kind of coffee do you like best?}{}
\item \utterance{Chatbot (anaphorized)} {What kind of \textbf{it} do you like best?}{}
\end{itemize}
\end{quote}

\subsubsection{Deep anaphora}

A deep anaphora is a situation
where the referent of the anaphoric device
occurs several utterances ago.
The depth is not measured by the number of occurrences
but by the number of new entities that occur
since the referent that the anaphora refers to.
Measuring the depth of anaphora by the number of utterances
does not capture the dynamic nature
of topic progression in the text of the conversation.
The number of utterances does not map
on how many topics have been visited.
While the number of new entities does not
map exactly either,
it is closer to the topic progression
and close to
what is being sought after here –
how far in the conversation is an entity
still acceptable or even available to speakers.
An entity can represent a topic
but can also be one of several entities to represent a topic
or can cover several topics at once
all depending on which way the conversation goes.

\par
As stated earlier,
both topic and entity are difficult to define
and their annotations tend to be recursive.
A close-enough approach has been adopted in this thesis.
While runtime topic annotation by an LLM
is not necessary for generating deep anaphora
and has therefore not been attempted in this thesis,
entity tracking is made possible by
entity recognition prompt.

This prompt runs in parallel with the next response generation
and writes down its results in the conversation status.
A chatbot that contains the deep anaphora stimuli
chooses a participant mentioned entity
relatively early on in the conversation
(though not at the very beginning)
and then tracks the newly mentioned entities.
When there have been 4 new entities mentioned,
the next response generation prompt is modified.
The modification lies in the context of the conversation
that has so far taken place is cut so
that the chatbot only has access
to the conversation until the point of the mention of the entity.
Given the response generation prompt
the response contains a question
about the mentioned entity.
Then the only thing that needs to be done
is modifying the response via the anaphorization prompt.

The trick here the participant and the chatbot differ in their perspectives
on what the conversation currently is.
The chatbot refers to something
that from the perspective of the participant
has been mentioned a while ago.

This approach is relatively imprecise and relies on luck to a certain degree.
Compared to the shallow anaphora,
it is expected to be somewhat more problematic
and perhaps cause the participant to request clarification.

\par
There has been one issue that has arisen while developing this stimulus
that has proven to alter the character of the data in an unwanted way.
Since the chatbot has no access to the conversation that happens
between the occurrence of the referred to entity and the participant's present moment,
chances are, the chatbot's question is on
a piece of information that has been mentioned in the meantime.
Whenever that happens, the degree of participant acceptance decreases significantly
due to a topical incoherence rather than
due to struggling to map a deep anaphora.
This has been dealt with by providing the chatbot
with the rest of the conversation in another component of the prompt
with the instruction to avoid any of the topics mentioned there.
LLMs are known to handle negative instructions with less success than positive ones\footcite{negated_prompts}
but this measure seems to have mitigated the problem
as can be seen in the data attached in the appendix.

\subsubsection{Non-assignable anaphora}

The last type of stimulus used in this thesis
has been called non-assignable anaphora.
It is a device that the participant tends to
interpret as an anaphoric device,
typically a personal or demonstrative pronoun,
but one such that the participant is not able
to map to any of the candidate entities
in the previous conversation text.
This stimulus is expected to lower the participant's acceptance
by the greatest amount.

To make a chatbot contain this stimulus
entities are tracked to make sure
there are candidates to be considered
in case an anaphora occurs.
Once there is a sufficient number of
entities recognized in the conversation
a hardcoded response is returned instead of an LLM-generated one.
The response contains a pronoun that
the participant needs to interpret as an anaphora to make sense of it.

Since the response containing the stimulus is hardcoded,
there is no guarantee
that it is incoherent with the previous conversation
and that there is no candidate to map the anaphora onto.
Though odds are high enough
every conversation that is supposed to contain this stimulus
have to be manually checked
to confirm the required stimulus is present.
This is the case for all the conversations, regardless,
because the presence of stimulus is not guaranteed
for shallow and deep anaphora either.

An approach not explored in this thesis is
achieving a non-assignable anaphora is also possible
via generating a response
using a prompt that instructs an LLM
to come up with a question containing an unrelated entity
avoiding all the mentioned entities
and anaphorize it before showing it to the participant.
Although LLMs tend to perform worse with negative instructions\footcite{negated_prompts}
this could be achieved using a chain of prompts.
The notion of non-assignable anaphora
brings into scope the question of
what makes an anaphora assignable.
It is the semantic compatibility of the words around the anaphora
that determines which of the candidates the anaphora is referring back to.
The generation of the lexical surroundings of the anaphora
needs to be handled carefully
when coming up with an LLM-based response.

\subsection{Ending the conversation}

While recognizing when the conversation is ending
or especially when it should not end
people rely on a set of clues
similarly to knowing when to take turns speaking.\footcite{closing1,davidson1975ending,coppock2005politeness}
In open–domain conversation
like the one a Convform chatbot holds with experiment participants,
the main challenge is to recognize
when there is a topic at hand
that interests the participant.
Another discipline in the realm of ending the conversation
is recognizing it is a good time to end the conversation
due to the participants' lack of interest or exhaustion of all topics at hand.
Conversation designs made for this thesis
do not take much of this into account.
The main goal for a Convform chatbot here
is to present the participant with a stimulus.
Once they manage that,
if the participant is willing to continue
the conversation continues for a hardcoded number of responses.
This leads to participants sometimes noticing
the conversation ending abruptly
and mentioning they would like to continue
in the questionnaire
or even at the very end of the conversation itself.
This can potentially have an effect on
the score given by the participant
and therefore brings noise into this parameter.

\par
Since each conversation has to be manually checked,
an assessment of how to deal with this noise can be made
while and after processing the data.
A runtime topic annotation
and other prompting techniques
could potentially help make
the Convform chatbot converses in such a way
that would be more aware of the general course of the conversation
perhaps giving hints about ending the conversation ahead of time
or reacting to and handling the participant's hints of the same type.

\chapter{Data}

The data collected using a convform chatbot
is a transcript of the conversation
between a participant and the chatbot.
Depending on the conversation design of the given chatbot
the conversation may contain a record of
the participant being exposed to a specific situation
and their reaction to it.
Other than that the collected data contains
an information about whether the participant quits the conversation,
the participants' rating of the conversation
and their comment on it.

Unfortunately it cannot be guaranteed
the required stimulus actually occurs in the conversation.
Though the probability is relatively high,
the LLM technology responsible for most answers
is nondeterministic
and participants tend to be unpredictable.
On many levels, conversation can take an unintended direction
which can spoil the stimulus.
Whether thematic, textual or interactional,
anything can go wrong.
That is why as mentioned earlier,
each conversation needs to be visited manually
to confirm required stimulus is present.
That does not present too much of a problem
since a qualitative analysis of the data
has the potential to bring lots of
corrections to experiment design,
relevant observations or
inspiration for further steps.
It would however pose a complication
was one interested in scaling up this approach
that would have to be dealt with.

\section{Data collection}

\subsection{Experimental design}

Data collection took place with the use of six chatbots.
Each of them represents a possible combination of a 3x2 design.
The two design variables are anaphora type and conversation style.

The anaphora types are as discussed above a shallow, deep and unassignable.
There are two conversation styles. One, inquisitive,
where the instruction part of the prompt
would always instruct the LLM to pose a question.
The other, relaxed, would interleave this question instruction with
an instruction to comment on the conversation in a nonquestion way.
The conversation style variable has been deployed for purely explorative purposes.

All the chatbots had the same persona instruction except their names.
They would communicate in Czech with participants who are czech or slovak native speakers.
The first utterance in the conversation would be the chatbots and
would be hardcoded to contain greeting and self introduction.

They would be instructed to chat for several turns and then the stimulus would come.
After the stimulus the chatbot would continue for a couple more turns and then say goodbye.
The questionnare would then appear to the participant.
That is unless the participant aborted earlier, in which case the questionnare would appear immediately.
In some cases participants simply left the user interface leaving no extra information.
Other then that, participants have been asked to grade the conversation on scale 1 to 5
where 1 is most acceptable and 5 is least acceptable.

Timing of the prompt would differ based on the anaphora type.
For shallow type the chatbot simply converses for two turns and thereafter it starts tracking entities.
At first recognized entity the prompt fires off
prompting an utterance about the exctracted entity and anaphorizing it.
With deep anaphora similar thing happens,
but the chatbot waits several more entity occurences
before also prompting an utterance about the extracted entity and anaphorizing it.
Lastly the non-assignable anaphora has the same timing method as the shallow anaphora,
but the prompt itself is hardcoded to contain an anaphoric reference which is
very unlikely to be assignable.

In the experiment, the participant would be instructed to end the conversation,
if the chatbot's communication was not natural.
The concrete Czech word used was "přirozená komunikace".

\subsection{Participants}
The data has been collected in two waves.
The pilot wave generated 50 conversations
the participants for which have been recruited from the circles of the papers author.
In the attached full data
the identificators of these conversations have
the letter p for pilot appended at the end.
The second wave generated 325 conversations
the participants for which have been recruited from the students of the Charles University.
They are instructed to take two conversations with a week pause in the middle.
However since for this experiment explorative in nature contrast and timing are not critical
they are free to revisit the chatbot however they liked.
There is no attempt made to indicate unique participants in the conversation as
this would not bring any observable results.
If such approach should be taken in the future,
more control has to be gained over the course of the converstaion
as discussed below.
No changes are made to the design for the second wave other then changing the structure of the qustionnare.
While in the pilot wave have been tasked to make a comment on the conversation or
explain why they aborted it if they did so,
the second wave has been shown the entire conversation
and particpant were been asked to mark and comment on specific chatbot responses they found odd or interesting.

\section{Data analysis}

Since the collected data is relatively complex
multiple layers of analysis need to be performed on it.
The steps of analysis are be following:

\begin{itemize}
\item{Conversation design stimulus annotation}
\item{Participant reaction quantitative analysis}
\item{General qualitative analysis}
\end{itemize}

In order to assess the datapoints for quantitative analysis
each conversation needs to be visited
to confirm or deny the presence of expected stimulus.
This preliminary step of qualitative analysis
provides feedback on whether the conversation design
that is meant to facilitate the experimental design
was succesful in doing so.
Further analysis leading up to a quantitative assessment
should only be done on conversations
where convform succesfully exposed the participant to the stimulus.
It has been observed in collected data
that sometimes an unintended stimulus
takes place instead of the intended one.
Then simply changing the label of the conversation is appropriate.
It is also necessary to filter the collected data for
noncooperative participants who make it impossible for convform
to perform the stimulus in the first place.
The participant needs to take on a role of a friendly conversator
if the experiment is to work.
If they for example attempt to take advantage of the LLMs obedience
and give it an unrelated task that involves any sort of text generation,
they are pronounced noncooperative.

The qualitative analysis of the participant reaction to stimulus
is mostly of explorative nature.
The convform environment lets the user to input any text
which even under the condition that they are cooperative
can be unexpected and can derail the conversation.
While these cases cannot be used to make generalizations
they need to be taken into account
as a possible participant behavior.
From the perspective of dialog system development
such cases would be considered to be edge cases
and dealing with them on the conversation design level
would probably be considered lower priority.
However the conversation analysis perspective
is interested in all the potential paths that
a certain conversation situation can go in.
This is ultimately why any weakness to the conversation design
does not matter too much
as all conversations are visited anyway
in search for unique situations and effort to understand full consequences of the conversation design.

In conclusion, the main reason for using
an experiment environment solution like convform
can be summed up in two points:

\begin{itemize}
\item{Partial control over what happens in the conversation}
\item{The ability to expose multiple people to a comparable conversational stimuli}
\end{itemize}

The first point has been described in detail above.
As far as the second point goes,
this approach is a step towards
a quantitative analysis of conversation.
Though conversation is multifactored
and various types of situation
always come with a different set of circumastances
if a quantitative analysis of a set of conversations
containing a certain reoccuring pattern
proves contrastive in some of the quantifiable parameters
to a set of conversations containing a different reoccuring pattern
case could be made this is caused by the observed patterns
regardless of all the noise naturally present in a conversation text.

\section{Annotation}

All 375 conversations have been visited to confirm whether
the stimulus expected based on the conversation design.
Other than that the participant reaction was annotated to be either
continuation, metacommunication or aborting of the conversation.
These three types of participant reaction to stimulus
represent the amount of understanding the reference.
In simple terms, continuation means
the stimulus went under the participants radar
or has been accepted.
Metacommunication, an utterance which somehow addresses the ongoing communication,
shows the participant not understanding,
but perhaps attributing the misunderstanding to themselves or
believing the issue can be resolved.
Lastly the conversation is aborted when the participant looses trust
in the ongoing conversation being coherent now or in the future.
Then the illusion of social interaction falls
and the conversation the participant is having dissolves into
typing text into a computer program and getting text in return.
The immersivness necessary for simulating a conversation is here no longer achieved.

\subsection{Inter--annotator agreement}

To make sure that annotating the data is possible,
inter-annotator agreement has been measured on 37 conversations which gives 10% of the data.
Three annotators annotated each conversation for anaphora type (shallow, deep, unassignable) and
whether the participant reaction to stimulus is metacommunicative.
Metacommunication is the only reaction type that needs manual annotation since
aborting the conversation is
observable in the transcript and marked in the data automatically
and any reaction that is not aborting and metacommunication has to be a continuation.

\subsubsection{Stimulus annotation}

For anaphora type Cohen's Kappa was calculated for each of the three annotator pairs
with following results:

\begin{itemize}[label={}]
    \item Cohen's Kappa for Annotator 1 and Annotator 2: 0.89
    \item Cohen's Kappa for Annotator 1 and Annotator 3: 0.88
    \item Cohen's Kappa for Annotator 2 and Annotator 3: 0.92
\end{itemize}

Coming to the average Pairwise Cohen's Kappa of 0.90
the inter-annotator agreement appears to be high enough
that this annotation categorization can be used and trusted.

Each of the conversations that caused disagreement is interesting in its own right:


\begin{quote}
\begin{itemize}[label={}, leftmargin=0pt, itemsep=0.5em]
    \item \textbf{conversation id: 145}
    \item ...
    \item \utterance{b:}{Jaké dárky plánuješ koupit?}{Which gifts do you intend to buy?}
    \item \utterance{h:}{ještě nevím, potřebuji pro taťku nějaký dárek a pro mamku}{don't know yet, need for dad a gift and for mom}
    \item \utterance{b:}{To může být výzva, ale určitě najdeš něco skvělého.}{That can be a challenge, but surely you'll find something great.}
    \item \utterance{h:}{jo, taky si myslím}{yeah, i also think}
    \item \utterance{b:}{Jaké koníčky má tvůj táta a mamka?}{Which hobbies does your dad a mom have?}
    \item \utterance{h:}{nevím, neznám je}{don't know, don't know them}
    \item \utterance{b:}{To je škoda, můžeš se jich zeptat.}{That is a shame, you can ask them.}
    \item \utterance{h:}{bydlí moc daleko}{they live too far}
    \item \utterance{b:}{Bot: To je těžké, ale co si zjistit jejich zájmy?}{That is tough, but how about finding out what their interests are?}
    \item \utterance{h:}{taky nevím§}{also don't know}
    \item \utterance{b:}{Možná bys mohl zjistit, co mají rádi.}{maybe you could look into, what they like}
    \item \utterance{h:}{ok}{ok}
    \item \utterance{b:}{To je dobrý nápad, jak to uděláš?}{That's a good idea, how will you do it?}
    \item \utterance{h:}{zeptáím se tety, ta určitě bude vědět}{I'll ask my aunt, she'll surely know}
    \item \utterance{b:}{Jaké mají \textbf{oni} koníčky?}{Which hobbies do they have?}
    \item \utterance{h:}{kdo?}{who?}
    \item ...
\end{itemize}
\end{quote}

This conversation potentially has a deep anaphora in it,
since the first occurence of the mappable entity \textit{dad and mom} is deep in the conversations history
at the stimulus time.
There are however other entites that are mappable
that are mentioned in the meantime like \textit{aunt}
or the general group of \textit{close ones} mentioned before the snippet starts.
This points at the potentially missing category of \textit{ambivalent anaphora}.
Another issue with this stimulus is that it askes a question that is already being discussed.
This can be seen as a both topical and sequentical coherence disruption.
Why would the same question with the same reference in it be posed?
That also makes the anaphora that much more vague and leads to a metacommunication \textit{who?}.

\begin{quote}
\begin{itemize}[label={}, leftmargin=0pt, itemsep=0.5em]
\item \textbf{conversation id: 150}
\item ...
\item \utterance{h:}{Běhám 400m překážek a dělám sedmiboj}{I run 400m with obstacles and do heptathlon}
\item \utterance{b:}{To je náročné! Jak dlouho už se tomu věnuješ?}{That's demanding! How long have you been doing this?}
\item ... 7 turns (14 utterances) ...
\item \utterance{h:}{Jsem po zánětu v patě, tak snad se to nevrátil, hodně to uvolňuji a cvičím na to}{I had a heel inflammation, hopefully it won't return, I release and excersise it a lot}
\item \utterance{b:}{Jaké disciplíny tvoří \textbf{on}? Jakou máš mezi nimi nejoblíbenější?}{Which disciplines does it consist of? Which is you favorite among them?}
\item \utterance{h:}{Kdo?}{Who?}
\item ...
\end{itemize}
\end{quote}

Here the mappable entity \textit{heptathlon} is so far deep in the conversation that
the annotator who would not agree with calling a deep anaphora stimulus
perhaps did not even notice it or insisted that this is just too deep.
While that is technically against the annotators instruction based on the above description of stimulus
but it points at the interesting problematic of anaphora depth.

The other two conversations that had an annotation disagreement about them are ids 151 and 153.
Both of them have an imperfect generation in them that excludes them from the stimuli containing data.

\subsubsection{Metacommunication annotation}

Inter-annotator agreement regarding the metacommunication has produced following results:

\begin{itemize}[label={}]
\item Cohen's Kappa for Annotator 1 and Annotator 2: 0.94
\item Cohen's Kappa for Annotator 1 and Annotator 3: 0.88
\item Cohen's Kappa for Annotator 2 and Annotator 3: 0.94
\end{itemize}

With the average Pairwise Cohen's Kappa coming to 0.92,
it can be considered reliable to annotate metacommunication
in the collected data.

The two conversations that had disagreement about them in terms of
presence of metacommunication in the participant reaction to stimulus
are both interesing in their own right.


\begin{quote}
\begin{itemize}[label={}, leftmargin=0pt, itemsep=0.5em]
\item \textbf{conversation id: 164}
\item ...
\item \utterance{b:}{Jakou písničku z \textbf{ní} máš nejraději?}{Which song from here do you like the best?}
\item \utterance{h:}{z rádia?}{from the radio?}
\item ...
\end{itemize}
\end{quote}

This reaction can be seen to contain an ellided metacommunicative device like \textit{do you mean}.
Since it is not present explicitely and the participant reaction is topically anchored in the conversation,
the case is ambivalent.

\begin{quote}
\begin{itemize}[label={}, leftmargin=0pt, itemsep=0.5em]
\item \textbf{conversation id: 173}
\item ...
\item \utterance{b:}{Tak to by mě zajímalo, jak na to pak \textbf{ona} zareagovala.}{Then I wonder, how she reaction to that.}
\item \utterance{h:}{Ano, pokazilo}{Yes, it went wrong}
\item ...
\end{itemize}
\end{quote}

It is unclear where the participant is coming from in their response.
For this reason, it is also unclear whether the participant reaction is a metacommunication.
There is a good chance the participant is somehow commenting on the chatbot breaking,
in which case such utterance would indeed be considered to address the ongoing communication.

\par
In conclusion, the annotator agreement is
strong enough for both stimulus type and metacommunication reaction and
quantitative analysis can be performed on the measured metrics.





\input{draft-final-chap.tex}

\chapter{Final thoughts}

\section{Evaluation of anaphora categorization}

    The categorization used in this research has been simplified for exploration.
    The immediate observation has been that both deep and non-assignable anaphora
    have two dominant participant reactions.
    The deep anaphora had a similar number of acceptance and metacommunicative reaction.
    This begs a question whether it is possible to program and prompt two different chatbots,
    that are capable of splitting these two types of reactions.
    In other words the question is whether it is possible to simulate an immediately acceptable
    and a confusing deep anaphora.
    The same goes for the non-assignable anaphora
    where the split was on the metacommunication and aborting the conversation.
    Here the task to split these two types up among two different chatbots is even trickier,
    because the criteria of not understanding and directly ending the conversation might depend on
    individual circumstances.
    That way the textual aspect of the conversation would have a lesser effect.

    That qualitative analysis shows also that
    at least one disruptive anaphora type has been elided from the categorization.
    In many cases the participant confusion stemmed from the ambivalence of the anaphora -
    there has been multiple mapping candidates.
    There is a hint of a more granular categorization of disruptive anaphora.

    Finally on anaphora and conversation coherence in general,
    it is not easy to break only the anaphora,
    something always comes along with it as the various factors at hand in conversation are intertwined.
    The data certainly showed, that everytime there is a anaphora related coherence disruption in conversation,
    this impacts the topical structure of the unfolding text of conversation
    which is where the incoherence ends up being perceived.

\section{Further steps}


    \subsection{Experiment design improvement}

        For future research better control over the conversation needs to be gained.
        That way the amount of noise can be decreased as there would be less sources
        of confusion or nonacceptability in the conversation text other than the stimulus.
        Participants could also be exposed to a sequence of conversations with different stimulus each time.
        That way the stimuli could be ordered a certain way and a rating development metric would be gained.
        Lastly more granular approach to the participant reaction to stimulus could be taken.
        At least one type has been ommited or rather fused with the metacommunication reaction type -
        the late aborting of the conversation.
        After the stimulus if the participant reacts in a metacommunicative way,
        the chatbot has a chance to fix the disruption.
        If it does not succeed in that, the coherence remains disrupted and
        the social interaction illusion falls due to the stimulus but not directly after the stimulus.
        For this the chatbot also needs to have access to which entity the anaphora has been referring to.

    \subsection{Stimulus generation improvement}

        Some of the challenges met during the course of the research have to do with
        the stimulus generation.
        One improvement that has been mentioned above is the chatbot being aware of what it is referring to
        with its anaphoras.
        In this version of convform, this information has not been stored anywhere.
        It should be present in the prompt in case the participant is confused and asking about it.
        The secondary reaction might show which types of referents are acceptable or less so.

        The conversation quality needs to improve.
        The zero anaphora has to be deployed where it is appropriate to decrease the nonacceptability noise.
        Better topic progression awareness needs to be developed in convform.
        The chatbot sometimes ignores questions and asks its own which is topically and sequentially inappropriate.

        Lastly the non-assignable anaphora stimulus could be improved to generate stimulus-responses via prompt.
        That would require generating an irrelevant entity and anaphorically referring to it.
        This challenge could provide more insight on associative anaphora if tackled succesfully.

    \subsection{Hybrid approach}

            All of the aforementioned improvements are hard to achieve with the current state of convform.
            At the time of this research it functions as a hybrid tree structure where the response is either
            hardcoded or a prompt.
            To gain more flexibility, the dialog management needs to be more prompt oriented.
            However to keep the control, the prompt needs to consist of components that self regulate based on
            logic and analysis prompts.
            Essentially with each turn metadata
            like topical and sequential status of the conversation should be collected.
            The components should then be allowed to assemble the prompt
            based on this data while following the conversation designer instruction.
            This process should allow for creating a dialog system that is aware of what is going on
            and can conditionally produce prompts and generate responses
            that are defined by the conversation designer
            while maintaining the flexibility with which state-of-the-art systems interact.

\section{New opportunities}

With a dialog system of this nature
intentionally creating the stimulus hinted at in the analysis would become possible:

    \begin{itemize}
    \item{Understanding the bimodality of deep and non-assignable anaphora}
    \item{Deep anaphora referring to a (non)topicalized entity}
    \item{Closer look at zero reference and associative anaphora}
    \item{Observing participant reactions to nonfactual implications}
    \item{Comparing different anaphora depths}
    \item{Topic shift with or without discourse markers}
    \item{Secondary participant reactions to repair attempts}
    \end{itemize}

\section{Conclusion}

The methodology used in present research has been experimental on two levels.
One was to see whether it can be useful for conversation research.
While there are many improvements to be made on both the experimental framework
and the stimuli, it has proven to be able to generate data that would be otherwise
difficult to acquire.
Annotation is however necessary. Every conversation has to be visited to confirm
whether the required stimulus is actually present.
With improving conversation design, this might become a smaller issue
and with a certain degree of precision and a certain scale,
annotations might become less essential
but not for now and near future.
Seeing the interactions with ones own eyes has been the goal of the research regardless,
so annotating can also be perceived as a reward rather than a price.

\par
The other level was to expose participants to a chatbot that would subvert the course of the conversation.
This has shown to be succesful to a decent degree although here also it a lot of improvement to be made
on both practical and theoretical aspect.
The result itself has been more or less unsurprising and the experiment served mainly to generate baseline data.
From here, multiple research paths can be taken in both understanding the inner workings of conversation
and attempting to simulate various situations using the dialog system.

\printbibliography



\end{document}
